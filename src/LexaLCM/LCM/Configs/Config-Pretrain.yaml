

model:
  name: Architecture_LCM
  # hidden_size: 2048 # commented original value because it's bugged. ToDo: fix it.
  hidden_size: 1024
  num_attention_heads: 16
  # num_contextualizer_layers: 5 # commented original value because it's too big to run while GPU is busy. ToDo: re-enable it.
  num_contextualizer_layers: 2
  #num_denoiser_layers: 13 # commented original value because it's too big to run while GPU is busy. ToDo: re-enable it.
  num_denoiser_layers: 4
  input_dim: 1024 # 1024 is the dimension of the SONAR embeddings
  activation_function: swiglu
  dropout: 0.1  # recommended default
  max_position_embeddings: 4096     # Needed for RoPE cache length

training:
  max_steps: 20000
  warmup_steps: 500
  max_grad_norm: 1.0
  log_every: 50
  eval_every: 1000
  batch_size: 16
  learning_rate: 2e-5
  weight_decay: 0.01
  bf16: true
  output_dir: ./outputs
  save_every: 1000
  checkpoint_dir: ./checkpoints
  # resume_from: ./checkpoints/checkpoint_step_5000.pt # Uncomment to resume from a checkpoint

data:
  data_dir: /home/lexa/DevProjects/_Models/LexaLCM_Pre1/src/LexaLCM/Content/Datasets/Wikipedia_Ja
  text_column: text_sentences_sonar_emb
  train_split: train
  val_split: validation

wandb:
  project: LexaLCM_Pre1
  run_name: lcm_run_001
  log_every: 50
